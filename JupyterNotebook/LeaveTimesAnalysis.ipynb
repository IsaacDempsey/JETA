{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeaveTimeAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook only deals with handling the big data file of leave time analysis and includes any python code that can be used to recreate data required for the Website, Model and Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Alot of code is commented out to save unnecessary execution time\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sc)\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To check the same minimum versions in both driver and worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the leavetime files\n",
    "df_16 = spark.read.option(\"delimiter\",\";\").csv(\"rt_leavetimes_2016_I_DB.txt\",header=True)\n",
    "df_17 = spark.read.option(\"delimiter\",\";\").csv(\"rt_leavetimes_2017_I_DB.txt\",header=True)\n",
    "\n",
    "# This function performs reduce function and returns a new df with all the rows. Since this is an action and not a lazy transformation \n",
    "# this fucntion can take time depending on memory and size of dfs\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unionAll(df_16,df_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Day of Week\n",
    "from_pattern=\"dd-MMM-yy HH:mm:ss\"\n",
    "to_pattern=\"EEEE\"\n",
    "from pyspark.sql.functions import unix_timestamp,from_unixtime\n",
    "df_dow = df.withColumn('dayOfWeek', from_unixtime(unix_timestamp(df['dayofservice'], from_pattern), to_pattern))\n",
    "### CHECKING WHAT MONTH AND YEAR OF DATA WE HAVE\n",
    "# to_pattern=\"MMM-yy\"\n",
    "# df_month = df.withColumn('MONTHYEAR', from_unixtime(unix_timestamp(df['dayofservice'], from_pattern), to_pattern))\n",
    "# pdf_month = df_month.select('MONTHYEAR').distinct().toPandas()\n",
    "# print(pdf_month.sort_values(by='MONTHYEAR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################-----------WEATHER--------------#######################################\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "#####################################################################################################\n",
    "# Till now we were working only with certain functions of pyspark.sql; We will import all the functions here so that we \n",
    "# can use any of them in future\n",
    "from pyspark.sql.functions import *\n",
    "# Since PySpark Dataframes are immutable it is not possible to make inplace transformations, hence we will split our column creation\n",
    "temp_date = df_dow.withColumn('date', from_unixtime(unix_timestamp(df_dow['dayofservice'], from_pattern), \"dd-MMM-yyyy\"))\n",
    "# df_dow.select(from_unixtime(df_dow['plannedtime_arr'],format=\"HH:00:00\")).show(2,truncate=True)\n",
    "temp_hour = temp_date.withColumn('hour',from_unixtime(temp_date['plannedtime_arr'],format=\"HH:00:00\"))\n",
    "temp_hr_dt = temp_hour.withColumn('dateNHour',concat(col(\"date\"),lit(\" \"),col(\"hour\")))\n",
    "# There are some extra columns created in our temp_hr_dt, before we submit our dataframe lets drop those\n",
    "# temp_hr_dt.drop('date').collect() Collect is taking too long hence we will create a new dataframe with our columns\n",
    "df_hour = temp_hr_dt.select('dayofservice','tripid','progrnumber','stoppointid','plannedtime_arr','plannedtime_dep','actualtime_arr','actualtime_dep','dayOfWeek','dateNHour')\n",
    "# Creating a pandas dataframe\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('postgresql+psycopg2://postgres:00001234@localhost:5433/jetaDb')\n",
    "df_weather = pd.read_sql_query(\"SELECT concat_ws(' ', date::text, time::text) AS date, rain, temp FROM main_weather;\",engine)\n",
    "# Converting the pandas dataframe to spark dataframe\n",
    "sdf_weather=sqlContext.createDataFrame(df_weather)\n",
    "from_pattern=\"MM/dd/yyyy HH:mm\"\n",
    "to_pattern=\"dd-MMM-yyyy HH:00:00\"\n",
    "# sdf_weather.select(from_unixtime(unix_timestamp(sdf_weather['date'], from_pattern), to_pattern)).show()\n",
    "df_weathers = sdf_weather.withColumn('dateJoiner',from_unixtime(unix_timestamp(sdf_weather['date'], from_pattern), to_pattern))\n",
    "df_weather_fin = df_weathers.drop('date')\n",
    "# df_weather_fin.show(5,truncate=True)\n",
    "# Both the dataframes are prepared. Left join\n",
    "df_weather_full = df_hour.join(df_weather_fin, df_hour[\"dateNHour\"]==df_weather_fin[\"dateJoiner\"],\"left_outer\")\n",
    "df_final = df_weather_full.select('dayofservice','tripid','progrnumber','stoppointid','plannedtime_arr','plannedtime_dep','actualtime_arr','actualtime_dep','dayOfWeek','rain','temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################----------------HOLIDAY-----------------################################\n",
    "###############################################################################################\n",
    "holidays_series = pd.read_sql_query(\"select date from main_bankholidays;\",engine)\n",
    "holidays = holidays_series['date'].tolist()\n",
    "import datetime\n",
    "holidays_new=[]\n",
    "for i,holiday in enumerate(holidays):\n",
    "    holidays_new.append(datetime.datetime.strptime(holiday, '%m/%d/%Y').strftime('%d-%b-%y %H:%M:%S'))\n",
    "    holidays_new[i] = holidays_new[i].upper()\n",
    "df_ready = df_final.withColumn(\"dayOfWeek\",when(col(\"dayofservice\").isin(holidays_new), \"Sunday\").otherwise(col(\"dayOfWeek\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################-------------TRIPS DATA------------####################################\n",
    "################################################################################################\n",
    "# Read the trips data\n",
    "df_16_trips = spark.read.option(\"delimiter\",\";\").csv(\"rt_trips_2016_I_DB.txt\",header=True)\n",
    "df_17_trips = spark.read.option(\"delimiter\",\";\").csv(\"rt_trips_2017_I_DB.txt\",header=True)\n",
    "# Merge the two dataframes\n",
    "df_trips_raw = unionAll(df_16_trips,df_17_trips)\n",
    "df_trips_tidy = df_trips_raw.selectExpr(\"dayofservice as Trips_dayofservice\",\"tripid as Trips_tripid\",\"lineid as lineid\",\"routeid as routeid\",\"direction as direction\",\"plannedtime_arr as Trips_plannedtime_arr\",\"plannedtime_dep as Trips_plannedtime_dep\",\"actualtime_arr as Trips_actualtime_arr\",\"actualtime_dep as Trips_actualtime_dep\")\n",
    "# from_pattern=\"dd-MMM-yy HH:mm:ss\"\n",
    "# to_pattern=\"MMM-yy\"\n",
    "# df_trips_month = df_trips_tidy.withColumn('MONTHYEAR', from_unixtime(unix_timestamp(df_trips_tidy['Trips_dayofservice'], from_pattern), to_pattern))\n",
    "# pdf_trips_month = df_trips_month.select('MONTHYEAR').distinct().toPandas()\n",
    "### MERGE LEAVE TIMES AND TRIPS\n",
    "# df_final = df_ready.join(df_trips_tidy, df_ready[\"tripid\"]==df_trips_tidy[\"Trips_tripid\"],how=\"inner\")\n",
    "# Merging df_leaveTimes and df_trips based on trip id and dayofservice\n",
    "# condition = [df_ready.tripid == df_trips_tidy.Trips_tripid, df_ready.dayofservice==df_trips_tidy.Trips_dayofservice]\n",
    "# df_final = df_ready.join(df_trips_tidy, condition, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################ ANALYZING PLANNED TIMES WITH RTPI TIMETABLE ############################\n",
    "# pdf_15B = df_final.where(df_final['lineid']=='15B').toPandas()\n",
    "# pdf_15B_JAN = pdf_15B[pdf_15B['dayofservice'].str.contains('JAN-17')]\n",
    "# pdf_15B_JAN_WEEK = pdf_15B_JAN[(pdf_15B_JAN['dayOfWeek']!='Saturday') & (pdf_15B_JAN['stoppointid']=='2917') & (pdf_15B_JAN['dayOfWeek']!='Sunday')]\n",
    "# pdf_15B_JAN_SAT = pdf_15B_JAN[(pdf_15B_JAN['dayOfWeek']=='Saturday') &  (pdf_15B_JAN['stoppointid']=='2917')]\n",
    "# pdf_15B_JAN_SUN = pdf_15B_JAN[(pdf_15B_JAN['dayOfWeek']=='Sunday') & (pdf_15B_JAN['stoppointid']=='2917')]\n",
    "# ## GETTING TIMETABLE RTPI\n",
    "# from pandas.io.json import json_normalize\n",
    "# import datetime\n",
    "# import requests\n",
    "# import json\n",
    "# stopid = \"2917\"\n",
    "# lineid = \"15B\"\n",
    "\n",
    "# timetable_request = requests.get(\"https://data.smartdublin.ie/cgi-bin/rtpi/timetableinformation?type=week&stopid={0}&routeid={1}\".format(stopid, lineid))\n",
    "# timetableJson = json.loads(timetable_request.text)\n",
    "# timetable = json_normalize(timetableJson['results'])\n",
    "# timetable = timetable[['lastupdated', 'startdayofweek', 'enddayofweek', 'destination', 'destinationlocalized', 'departures']]\n",
    "# # timetable\n",
    "# #########____________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testPDF15BWEEK = pdf_15B_JAN_WEEK[['stoppointid','dayOfWeek','plannedtime_arr']]\n",
    "# nodup = testPDF15BWEEK.drop_duplicates()\n",
    "# nodup.sort_values(by='plannedtime_arr')\n",
    "# nodup['humTime'] = pd.to_datetime(nodup['plannedtime_arr'], unit=\"s\")\n",
    "# # nodup['humTime'] = nodup['humTime']  + pd.Timedelta('01:00:00')\n",
    "# nodup['Time'] = nodup.humTime.dt.time\n",
    "# nodup.drop(['stoppointid','dayOfWeek','plannedtime_arr','humTime'],axis=1,inplace=True)\n",
    "# nodup.drop_duplicates('Time',inplace=True)\n",
    "# # nodup.sort_values('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime = nodup.sort_values('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week=[]\n",
    "# for i,row in enumerate(timetable['departures']):\n",
    "#     if i == 2:\n",
    "#         week=row\n",
    "# df_15B_2917_WeekDay_TimeTable_RTPI=pd.DataFrame({'Time':week})\n",
    "# df_15B_2917_WeekDay_TimeTable_RTPI.sort_values(['Time'],inplace=True)\n",
    "# df_15B_2917_WeekDay_TimeTable_RTPI.drop_duplicates('Time',inplace=True)\n",
    "# # len(df_15B_2917_WeekDay_TimeTable_RTPI)\n",
    "# pd.options.display.max_rows=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime.reset_index(drop=True,inplace=True)\n",
    "# # display(df_15B_2917_WeekDay_TimeTable_LeaveTime)\n",
    "# # display(df_15B_2917_WeekDay_TimeTable_RTPI)\n",
    "# display_side_by_side(df_15B_2917_WeekDay_TimeTable_LeaveTime,df_15B_2917_WeekDay_TimeTable_RTPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime = pd.DataFrame({'Time':nodup1['']})\n",
    "# week=[]\n",
    "# for i,row in enumerate(timetable['departures']):\n",
    "#     if i == 2:\n",
    "#         week=row\n",
    "# from datetime import datetime\n",
    "# from time import mktime\n",
    "# weekFromData = []\n",
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime['tt'] = df_15B_2917_WeekDay_TimeTable_LeaveTime['Time'].dt.time\n",
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime\n",
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime['tt']\n",
    "# weekFromData = []\n",
    "# for i,row in enumerate(df_15B_2917_WeekDay_TimeTable_LeaveTime['tt']):\n",
    "#     stri = str(row)\n",
    "#     hm = stri[:-2]+\"00\"\n",
    "#     weekFromData.append(hm)\n",
    "# # Finding common times\n",
    "# # len(weekFromData) # = 60\n",
    "# finalData = []\n",
    "# for vals in week:\n",
    "#     for vals2 in weekFromData:\n",
    "#         if vals == vals2:\n",
    "#             finalData.append(vals)\n",
    "#         else:\n",
    "#             continue\n",
    "# finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import mktime\n",
    "weekFromData = []\n",
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime['Minute'] = df_15B_2917_WeekDay_TimeTable_LeaveTime.Time.dt.minute\n",
    "# df_15B_2917_WeekDay_TimeTable_LeaveTime['Hour'] = df_15B_2917_WeekDay_TimeTable_LeaveTime.Time.dt.hour\n",
    "# ttWeek['FromTTHour'] = ttWeek.dtTime.dt.hour\n",
    "# ttWeek['FromTTMinute'] = ttWeek.dtTime.dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# AFTER JOIN, Data reduces from 92mil to 82 mil #####################################\n",
    "# ####################################################################################################\n",
    "# # count_merged_df_final = df_final.count()\n",
    "# df_final.printSchema()\n",
    "# # print(count_merged_df_final)\n",
    "# ####################### CHECKING WHY JOIN RESULTS IN 10 MILLION LESS RECORDS\n",
    "# # df_exception = df_final.select(\"dayofservice\",\"tripid\",\"progrnumber\",\"stoppointid\",\"plannedtime_arr\",\"plannedtime_dep\",\"actualtime_arr\",\"actualtime_dep\",\"dayOfWeek\",\"rain\",\"temp\")\n",
    "# # df_exception.printSchema()\n",
    "# # df_ready.printSchema()\n",
    "# # df_ready.subtract(df_exception).show() # Subtract from total\n",
    "# # df_leftout = df_ready.subtract(df_exception)\n",
    "# # df_leftout.count() # Number of stops that have journey segment data and no journey data\n",
    "# df_leftout_count=9627175 # Manually storing value for record purposes\n",
    "# ######################## CHECKING THE DIFFERENCES IN DAY AND MONTH DATA BETWEEN TRIPS AND LEAVETIMES ##############\n",
    "# # df_trips_ordered_month = df_trips_tidy.orderBy('Trips_dayofservice')\n",
    "# # df_trips_ordered_month.count()\n",
    "# # df_trips_ordered_month.show()\n",
    "# ############# IT SEEMS THAT LEAVETIME DATA DOES NOT HAVE DATA FROM JANUARY TO MARCH FOR YEAR 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ###################### CREATING DATA FOR ROUTES TABLE #######################\n",
    "# df_final.printSchema()\n",
    "# df_routes = df_final.select('routeid','stoppointid','progrnumber','direction')\n",
    "# df_routes_final = df_routes.distinct()\n",
    "# # df_routes_final.orderBy('progrnumber').show(20,truncate=True)\n",
    "# # df_routes_final.count()\n",
    "# # routes_pdf = df_routes_final.toPandas()\n",
    "# # len(routes_pdf['stoppointid'].unique())\n",
    "# # df_routes_totTable = pd.DataFrame({'Routes':routes_pdf['routeid'].unique()})\n",
    "# # len(df_routes_totTable)\n",
    "# # routes_pdf['progrnumber'].astype('int32',inplace=True)\n",
    "# # routes_pdf[['progrnumber','direction','stoppointid']] = routes_pdf[['progrnumber','direction','stoppointid']].astype('int')\n",
    "# routes_pdf_sorted = routes_pdf.sort_values(by=['routeid','progrnumber'])\n",
    "# routes_pdf_final_Tablle_Load = routes_pdf_sorted.groupby('routeid',as_index=False).agg(lambda x: x.tolist())\n",
    "# for i,r in enumerate(routes_pdf_final_Tablle_Load['direction']):\n",
    "# #     routes_pdf_final_Tablle_Load.set_value(i,'direction',r[0])\n",
    "# routes_pdf_final_Tablle_Load.drop(['progrnumber'],axis=1,inplace=True)\n",
    "# routes_final_table = routes_pdf_final_Tablle_Load.rename(columns={'stoppointid':'stopids'})\n",
    "# routes_final_table_new = routes_final_table[['routeid','stopids','direction']]\n",
    "# # import psycopg2\n",
    "\n",
    "# # routes_final_table_new.to_sql('main_routes',engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Unique Segements from the Routes table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Engine** --> DB Engine\n",
    "* **df_trips_tidy** --> Complete Trips Data\n",
    "* **df** --> Complete LeaveTimes Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  The following steps must be followed to make a file with segments\n",
    "* Make a new dataframe which has just the stopids, planned arrival time = sdf_model_timetable\n",
    "* Take a distinct of all these values \n",
    "* Make a new dataframe with segmentid, actualarrivaltime of the source and actual arrival time of destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data and creating the structure of our segmented journey logs\n",
    "# Now we have the unique trip for the route. We will now order the leavetimes file by tripid and then by time\n",
    "sdf_stops_ordered = df.select(['tripid','dayofservice','stoppointid','plannedtime_arr'])\n",
    "# Ordering the dataframe as per trips and plannedtime_arr\n",
    "sdf_stops_ordered = sdf_stops_ordered.orderBy(['tripid','plannedtime_arr'])\n",
    "\n",
    "# Display the ordered list\n",
    "# sdf_stops_ordered.show()\n",
    "# Performing some tests to make sure that we aren't losing on data\n",
    "# sdf_stops_ordered.where((col('plannedtime_arr').like(\"788%\"))&(col('tripid')=='2760214')).show()\n",
    "# sdf_stops_ordered.count()\n",
    "\n",
    "# Our ordered leavetime file seems fine. We will now move on to getting a dataframe that contains \n",
    "# the segmentid, routeid, tripid, plannedtime_arr at from, plannedtime_arr at to\n",
    "from pyspark.sql.types import * # We need this since we want to provide the schema for our new dataframe\n",
    "fields = [\n",
    "    StructField(\"dayofservice\",StringType(),True),\n",
    "    StructField(\"tripid\",StringType(),True),\n",
    "    StructField(\"time_at_1\",StringType(),True),\n",
    "    StructField(\"dayOfWeek\",StringType(),True),\n",
    "    StructField(\"rain\",StringType(),True),\n",
    "    StructField(\"temp\",StringType(),True),\n",
    "    StructField(\"time_at_2\",StringType(),True),\n",
    "    StructField(\"segmentid\",StringType(),True),\n",
    "    StructField(\"traveltime\",LongType())\n",
    "]\n",
    "schema = StructType(fields)\n",
    "sdf_stops_model = sqlContext.createDataFrame(sc.emptyRDD(),schema)\n",
    "\n",
    "\n",
    "# ## Creating the structure of from stops and df and to stops df\n",
    "# fields = [\n",
    "#     StructField(\"SegmentID\",StringType(),True),\n",
    "#     StructField(\"TripID\",StringType(),True),\n",
    "#     StructField(\"time_at_1\",StringType(),True),\n",
    "#     StructField(\"time_at_2\",StringType(),True)\n",
    "# ]\n",
    "# schema = StructType(fields)\n",
    "# sdf_stops_model = sqlContext.createDataFrame(sc.emptyRDD(),schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of segments and routes with the segments\n",
    "df_routes_for_segment = pd.read_sql_query(\"select * from main_routes;\",engine)\n",
    "segmentids = []\n",
    "routeids = []\n",
    "segmentnos = []\n",
    "for i, rows in df_routes_for_segment.iterrows():\n",
    "    for j in range(len(rows['stopids'])):\n",
    "        if j < (len(rows['stopids'])-1):\n",
    "            segment = str(rows['stopids'][j])+\"_\"+str(rows['stopids'][j+1])\n",
    "            segmentids.append(segment)\n",
    "            segmentnos.append(j+1)\n",
    "            routeids.append(rows['routeid'])\n",
    "        else:\n",
    "            break\n",
    "df_journey = pd.DataFrame({'routeid':routeids,'segmentno':segmentnos,'segmentid':segmentids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the duplicate segments -> Queueing Theory\n",
    "df_journey.drop_duplicates(subset='segmentid',keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_journey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_journey[df_journey['segmentid']=='4392_2446']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the code:\n",
    "# # Input values reading here\n",
    "# segments = '4392_2446' # The segment ID\n",
    "# route = '9_7' # The Route id\n",
    "# from_stop, to_stop = segments.split('_') # Splitting the segment into from stop and to stop\n",
    "# route_trips_unq = [val.Trips_tripid for val in df_trips_tidy.where(df_trips_tidy['routeid']==route).distinct().collect()]\n",
    "# from_stop_df = sdf_stops_ordered.selectExpr('dayofservice','tripid','plannedtime_arr as time_at_1','stoppointid').where((sdf_stops_ordered.tripid.isin(route_trips_unq))&(sdf_stops_ordered.stoppointid==from_stop))\n",
    "# from_stop_df = from_stop_df.drop('stoppointid')\n",
    "# to_stop_df = sdf_stops_ordered.selectExpr('dayofservice','tripid','plannedtime_arr as time_at_2','stoppointid').where((sdf_stops_ordered.tripid.isin(route_trips_unq))&(sdf_stops_ordered.stoppointid==to_stop))\n",
    "# to_stop_df = to_stop_df.drop('stoppointid')\n",
    "# # # condition = [from_stop_df.tripid == to_stop_df.tripid, from_stop_df.dayofservice==to_stop_df.dayofservice]\n",
    "# from pyspark.sql.functions import *\n",
    "# import pyspark.sql.functions as sf\n",
    "# from pyspark.sql.types import LongType\n",
    "# joined_df = from_stop_df.join(to_stop_df,['dayofservice','tripid'],'inner')\n",
    "# joined_df = joined_df.withColumn('traveltime',joined_df[\"time_at_2\"].cast(LongType())-joined_df[\"time_at_1\"].cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_stop_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_stop_df = from_stop_df.drop('stoppointid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_trips_unq = [val.Trips_tripid for val in df_trips_tidy.where(df_trips_tidy['routeid']=='120_8').distinct().collect()]\n",
    "# from_stop_df = sdf_stops_ordered.where((sdf_stops_ordered.tripid.isin(route_trips_unq))&(sdf_stops_ordered.stoppointid==from_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf_stops_ordered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_stop_df = from_stop_df.drop('stoppointid').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_trips = df_trips_tidy.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    StructField(\"tripid\",StringType(),True),\n",
    "    StructField(\"dayofservice\",StringType(),True),\n",
    "    StructField(\"stoppointid\",StringType(),True),\n",
    "    StructField(\"plannedtime_arr\",StringType(),True)\n",
    "]\n",
    "schema = StructType(fields)\n",
    "from_stop_df = sqlContext.createDataFrame(sc.emptyRDD(),schema)\n",
    "to_stop_df = sqlContext.createDataFrame(sc.emptyRDD(),schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf_stops_ordered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journey.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func(df):\n",
    "#     route_trips_unq = []\n",
    "#     for i, rows in df.iterrows():\n",
    "#         route = rows['routeid']\n",
    "#         print(i)\n",
    "#         route_trips_unq = (pdf_trips[pdf_trips['routeid'].isin([route])]['Trips_tripid'].tolist())\n",
    "#     return route_trips_unq\n",
    "\n",
    "# if __name__==\"__main__\":\n",
    "#     p = Pool(100)\n",
    "#     print(p.map(func,df_journey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# num_processes = multiprocessing.cpu_count()\n",
    "# chunk_size = int(df_journey.shape[0]/num_processes)\n",
    "# chunks = [df_journey.iloc[i:i + chunk_size,:] for i in range(0, df_journey.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func(d):\n",
    "#    # let's create a function that squares every value in the dataframe\n",
    "#    return d * d\n",
    "# # create our pool with `num_processes` processes\n",
    "# pool = multiprocessing.Pool(processes=num_processes)\n",
    "# # apply our function to each chunk in the list\n",
    "# result = pool.map(func, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_trips.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = pdf_trips[['Trips_tripid','routeid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_39A_46A_routes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_39A_routes = fin_df['routeid'].loc[fin_df['routeid'].str.startswith('39A')].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_46A_routes =fin_df['routeid'].loc[fin_df['routeid'].str.startswith('46A')].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journey.sort_values(by='routeid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we will run our loops:\n",
    "# First loop will iterate over the journey dataframe\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import LongType\n",
    "for i, rows in df_journey.iterrows():\n",
    "    route_trips_unq = []\n",
    "    segments = rows['segmentid']\n",
    "    route = rows['routeid']\n",
    "    if route in _39A_routes or route in _46A_routes:\n",
    "        route_trips_unq = fin_df[fin_df['routeid'].isin([route])]['Trips_tripid'].tolist()\n",
    "        print(i)\n",
    "        from_stop, to_stop = segments.split('_')\n",
    "        from_stop_df = sdf_stops_ordered.selectExpr('dayofservice','tripid','plannedtime_arr as time_at_1','stoppointid').where((sdf_stops_ordered.stoppointid==from_stop)&(sdf_stops_ordered.tripid.isin(route_trips_unq)))\n",
    "        to_stop_df = sdf_stops_ordered.selectExpr('dayofservice','tripid','plannedtime_arr as time_at_2','stoppointid').where((sdf_stops_ordered.stoppointid==to_stop)&(sdf_stops_ordered.tripid.isin(route_trips_unq)))\n",
    "        to_stop_df = to_stop_df.drop('stoppointid')\n",
    "        from_stop_df = from_stop_df.drop('stoppointid')\n",
    "        joined_df = from_stop_df.join(to_stop_df,['dayofservice','tripid'],'inner')\n",
    "        joined_df=joined_df.withColumn('segmentid',sf.lit(segments))\n",
    "        joined_df = joined_df.withColumn('traveltime',joined_df[\"time_at_2\"].cast(LongType())-joined_df[\"time_at_1\"].cast(LongType()))\n",
    "        sdf_stops_model = unionAll(sdf_stops_model,joined_df)\n",
    "    if route.startswith('5'):\n",
    "        break\n",
    "    # # condition = [from_stop_df.tripid == to_stop_df.tripid, from_stop_df.dayofservice==to_stop_df.dayofservice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_stop_df_old = from_stop_df\n",
    "to_stop_df_old = to_stop_df\n",
    "# joined_df = from_stop_df.join(to_stop_df,['dayofservice','tripid'],'inner')\n",
    "# joined_df=joined_df.withColumn('segmentid',sf.lit(segments))\n",
    "# joined_df = joined_df.withColumn('traveltime',joined_df[\"time_at_2\"].cast(LongType())-joined_df[\"time_at_1\"].cast(LongType()))\n",
    "# sdf_stops_model = unionAll(sdf_stops_model,joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding Day of week\n",
    "from_pattern=\"dd-MMM-yy HH:mm:ss\"\n",
    "to_pattern=\"EEEE\"\n",
    "from pyspark.sql.functions import unix_timestamp,from_unixtime\n",
    "sdf_stops_model = sdf_stops_model.withColumn('dayOfWeek', from_unixtime(unix_timestamp(sdf_stops_model['dayofservice'], from_pattern), to_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_stops_model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################-----------WEATHER--------------#######################################\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "#####################################################################################################\n",
    "# Till now we were working only with certain functions of pyspark.sql; We will import all the functions here so that we \n",
    "# can use any of them in future\n",
    "from pyspark.sql.functions import *\n",
    "# Since PySpark Dataframes are immutable it is not possible to make inplace transformations, hence we will split our column creation\n",
    "temp_date = sdf_stops_model.withColumn('date', from_unixtime(unix_timestamp(sdf_stops_model['dayofservice'], from_pattern), \"dd-MMM-yyyy\"))\n",
    "# df_dow.select(from_unixtime(df_dow['plannedtime_arr'],format=\"HH:00:00\")).show(2,truncate=True)\n",
    "temp_hour = temp_date.withColumn('hour',from_unixtime(temp_date['time_at_1'],format=\"HH:00:00\"))\n",
    "temp_hr_dt = temp_hour.withColumn('dateNHour',concat(col(\"date\"),lit(\" \"),col(\"hour\")))\n",
    "# There are some extra columns created in our temp_hr_dt, before we submit our dataframe lets drop those\n",
    "# temp_hr_dt.drop('date').collect() Collect is taking too long hence we will create a new dataframe with our columns\n",
    "df_hour = temp_hr_dt.select('dayofservice','tripid','time_at_1','time_at_2','traveltime','segmentid','dayOfWeek','dateNHour')\n",
    "# # Creating a pandas dataframe\n",
    "# from sqlalchemy import create_engine\n",
    "# import pandas as pd\n",
    "# engine = create_engine('postgresql+psycopg2://postgres:00001234@localhost:5433/jetaDb')\n",
    "# df_weather = pd.read_sql_query(\"SELECT concat_ws(' ', date::text, time::text) AS date, rain, temp FROM main_weather;\",engine)\n",
    "# # Converting the pandas dataframe to spark dataframe\n",
    "# sdf_weather=sqlContext.createDataFrame(df_weather)\n",
    "# from_pattern=\"MM/dd/yyyy HH:mm\"\n",
    "# to_pattern=\"dd-MMM-yyyy HH:00:00\"\n",
    "# # sdf_weather.select(from_unixtime(unix_timestamp(sdf_weather['date'], from_pattern), to_pattern)).show()\n",
    "# df_weathers = sdf_weather.withColumn('dateJoiner',from_unixtime(unix_timestamp(sdf_weather['date'], from_pattern), to_pattern))\n",
    "# df_weather_fin = df_weathers.drop('date')\n",
    "# # df_weather_fin.show(5,truncate=True)\n",
    "# # Both the dataframes are prepared. Left join\n",
    "df_weather_full = df_hour.join(df_weather_fin, df_hour[\"dateNHour\"]==df_weather_fin[\"dateJoiner\"],\"left_outer\")\n",
    "sdf_stops_modelfinal = df_weather_full.select('dayofservice','tripid','progrnumber','stoppointid','plannedtime_arr','plannedtime_dep','actualtime_arr','actualtime_dep','dayOfWeek','rain','temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################----------------HOLIDAY-----------------################################\n",
    "###############################################################################################\n",
    "holidays_series = pd.read_sql_query(\"select date from main_bankholidays;\",engine)\n",
    "holidays = holidays_series['date'].tolist()\n",
    "import datetime\n",
    "holidays_new=[]\n",
    "for i,holiday in enumerate(holidays):\n",
    "    holidays_new.append(datetime.datetime.strptime(holiday, '%m/%d/%Y').strftime('%d-%b-%y %H:%M:%S'))\n",
    "    holidays_new[i] = holidays_new[i].upper()\n",
    "sdf_stops_model_ready = sdf_stops_modelfinal.withColumn(\"dayOfWeek\",when(col(\"dayofservice\").isin(holidays_new), \"Sunday\").otherwise(col(\"dayOfWeek\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_stops_model = sdf_stops_model.drop('dayofservice, time_at_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_stops_model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling and saving the files\n",
    "percent_back = 0.15\n",
    "#This calculates the fraction of population that has to be included in the sample\n",
    "# 5% from each weekday \n",
    "frac = dict(\n",
    "    (e.dayOfWeek, percent_back) \n",
    "    for e \n",
    "    in sdf_stops_model.select('segmentid').distinct().collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = df_Full_LT.sampleBy('segmentid', fractions=frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled.repartition(1).write.partitionBy(\"segmentid\").csv(r'C:\\UCD\\RESEARCH\\Database DB\\Splits_Sample_Partitioned_SegmentId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_stop_df = to_stop_df.drop('stoppointid')\n",
    "from_stop_df = from_stop_df.drop('stoppointid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_stop_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = from_stop_df.join(to_stop_df,['dayofservice','tripid'],'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unionAll(sdf_stops_model,joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_stops_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf_stops_model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Getting all the trip id for route 120_8 from the trips file\n",
    "sdf_trips_from_route = df_trips_tidy.where(df_trips_tidy['routeid']==route)\n",
    "pdf_trips_from_route = df_trips_tidy.select('Trips_tripid').distinct().toPandas() # Contains All the trip ids for the specified route\n",
    "\n",
    "route_trips_unq = pdf_trips_from_route['Trips_tripid'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our Dataframe for the model is created. We have the following items with us:\n",
    "* Stops list ordered by tripids and time\n",
    "* to and from stops to make the segment\n",
    "* route id\n",
    "* List of unique trip ids for our route <br>\n",
    "\n",
    "\n",
    "> With the above available items we need to make dataframe as follows:\n",
    "* loop over the uniqiue trip id list and filter stops list by from stop and tripid\n",
    "* filter by to stop and trip id \n",
    "* append in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf_stops_model.show()\n",
    "# route_trips_unq\n",
    "# for trips in route_trips_unq:\n",
    "#### Checking if we are getting the results that we need\n",
    "# pt_cval = sdf_stops_ordered.select('plannedtime_arr').where((col('tripid')=='3005987')&(col('stoppointid')=='4381')).toPandas()\n",
    "# pt_cval2 = sdf_stops_ordered.select('plannedtime_arr').where((col('tripid')=='3005987')&(col('stoppointid')=='935')).toPandas()\n",
    "# from_stops_list = pd.DataFrame(columns=['plannedtime_arr'])\n",
    "# sdf_stops_ordered.filter(sdf_stops_ordered.tripid.isin(route_trips_unq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_stop_df_time = sdf_stops_ordered.where((sdf_stops_ordered.tripid.isin(route_trips_unq))&(sdf_stops_ordered.stoppointid=='4381'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as sf\n",
    "from_stop_df_time = from_stop_df_time.withColumn('tripid',sf.lit(route))\n",
    "from_stop_df_time = from_stop_df_time.withColumn('segmentid',sf.lit(segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_stop_df_time.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_trips_unq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf_model_timetable = df.describe() # Saved as LeaveTimeSummary.csv\n",
    "# pandas_leaveTImeDescription = sdf_model_timetable.toPandas()\n",
    "# pandas_leaveTImeDescription.T.to_csv('LeaveTimesSummary.csv')\n",
    "\n",
    "sdf_model_stopsData = df.select(['tripid','dayofservice','stoppointid','plannedtime_arr'])\n",
    "# sdf_model_stopsData.count() # 13,166,639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For everytrip we need an ordered arrangement by tripid and time so that we can have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_stop_temp_df = sdf_model_stopsData.where(sdf_model_stopsData['stoppointid']==from_stop)\n",
    "to_stop_temp_df = sdf_model_stopsData.where(sdf_model_stopsData['stoppointid']==to_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_stop_temp_df.toPandas()\n",
    "# to_stop_temp_df.toPandas()\n",
    "from_stop_temp_df = from_stop_temp_df.orderBy('plannedtime_arr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_stop_temp_df.count() # 1150 records\n",
    "to_stop_temp_df.count() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_stop_temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_DF = df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_final.select('dayofservice','dayOfWeek','tripid','routeid','direction','Trips_plannedtime_arr','Trips_plannedtime_dep','Trips_actualtime_arr','Trips_actualtime_dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_final.filter(df_final.Trips_tripid.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_count = filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.createOrReplaceTempView(\"filtered_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = spark.sql(\"select dayofservice, lineid,routeid,direction, stoppointid, progrnumber, plannedtime_arr, plannedtime_dep, actualtime_arr, actualtime_dep, Trips_plannedtime_arr, Trips_plannedtime_dep, Trips_actualtime_arr, Trips_actualtime_dep from filtered_tab order by dayofservice asc, routeid, direction, progrnumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDup_df = filtered_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying gpu gataframe\n",
    "from pygdf.dataframe import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "@pandas_udf(noDup_df.schema, PandasUDFType.GROUPED_MAP)\n",
    "def gpu_plus_one_grouped(pdf):\n",
    "    gpu_df = DataFrame.from_pandas(pdf)\n",
    "    gpu_df['oldcol'] = gpu_df['oldcol'] + 1\n",
    "    returnpgu_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDuplicates_DF = noDup_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDuplicates_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDup_df.createOrReplaceTempView(\"filtered_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = spark.sql(\"select dayofservice, lineid,routeid,direction, stoppointid, progrnumber, plannedtime_arr, plannedtime_dep, actualtime_arr, actualtime_dep, Trips_plannedtime_arr, Trips_plannedtime_dep, Trips_actualtime_arr, Trips_actualtime_dep from filtered_tab order by dayofservice asc, routeid, direction, progrnumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDuplicateGroup = df_group.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDuplicateGroup_count = noDuplicateGroup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(noDuplicateGroup_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noDuplicateGroup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

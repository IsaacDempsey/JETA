{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Spark Creation of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading leavetimes\n",
    "# Loading the leavetime files\n",
    "df_16 = spark.read.option(\"delimiter\",\";\").csv(\"rt_leavetimes_2016_I_DB.txt\",header=True)\n",
    "df_17 = spark.read.option(\"delimiter\",\";\").csv(\"rt_leavetimes_2017_I_DB.txt\",header=True)\n",
    "\n",
    "# This function performs reduce function and returns a new df with all the rows. Since this is an action and not a lazy transformation \n",
    "# this fucntion can take time depending on memory and size of dfs\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unionAll(df_16,df_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trips data\n",
    "df_16_trips = spark.read.option(\"delimiter\",\";\").csv(\"rt_trips_2016_I_DB.txt\",header=True)\n",
    "df_17_trips = spark.read.option(\"delimiter\",\";\").csv(\"rt_trips_2017_I_DB.txt\",header=True)\n",
    "# Merge the two dataframes\n",
    "df_trips_raw = unionAll(df_16_trips,df_17_trips)\n",
    "df_trips_tidy = df_trips_raw.selectExpr(\"dayofservice as Trips_dayofservice\",\"tripid as Trips_tripid\",\"lineid as lineid\",\"routeid as routeid\",\"direction as direction\",\"plannedtime_arr as Trips_plannedtime_arr\",\"plannedtime_dep as Trips_plannedtime_dep\",\"actualtime_arr as Trips_actualtime_arr\",\"actualtime_dep as Trips_actualtime_dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Unique Segements from the Routes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasource: string (nullable = true)\n",
      " |-- dayofservice: string (nullable = true)\n",
      " |-- tripid: string (nullable = true)\n",
      " |-- progrnumber: string (nullable = true)\n",
      " |-- stoppointid: string (nullable = true)\n",
      " |-- plannedtime_arr: string (nullable = true)\n",
      " |-- plannedtime_dep: string (nullable = true)\n",
      " |-- actualtime_arr: string (nullable = true)\n",
      " |-- actualtime_dep: string (nullable = true)\n",
      " |-- vehicleid: string (nullable = true)\n",
      " |-- passengers: string (nullable = true)\n",
      " |-- passengersin: string (nullable = true)\n",
      " |-- passengersout: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- suppressed: string (nullable = true)\n",
      " |-- justificationid: string (nullable = true)\n",
      " |-- lastupdate: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_stops_ordered = df.drop('datasource','plannedtime_dep','actualtime_arr','actualtime_dep','vehicleid','passengers','passengersin','passengersout','distance','suppressed','justificationid','lastupdate','note')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_stops_ordered = sdf_stops_ordered.orderBy(['tripid','plannedtime_arr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * # We need this since we want to provide the schema for our new dataframe\n",
    "fields = [\n",
    "    StructField(\"dayofservice\",StringType(),True),\n",
    "    StructField(\"tripid\",StringType(),True),\n",
    "    StructField(\"time_at_1\",StringType(),True),\n",
    "    StructField(\"time_at_2\",StringType(),True),\n",
    "    StructField(\"segmentid\",StringType(),True),\n",
    "    StructField(\"traveltime\",LongType())\n",
    "]\n",
    "schema = StructType(fields)\n",
    "sdf_stops_model = sqlContext.createDataFrame(sc.emptyRDD(),schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('postgresql+psycopg2://postgres:00001234@localhost:5433/jetaDb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of segments and routes with the segments\n",
    "df_routes_for_segment = pd.read_sql_query(\"select * from main_routes;\",engine)\n",
    "segmentids = []\n",
    "routeids = []\n",
    "segmentnos = []\n",
    "for i, rows in df_routes_for_segment.iterrows():\n",
    "    for j in range(len(rows['stopids'])):\n",
    "        if j < (len(rows['stopids'])-1):\n",
    "            segment = str(rows['stopids'][j])+\"_\"+str(rows['stopids'][j+1])\n",
    "            segmentids.append(segment)\n",
    "            segmentnos.append(j+1)\n",
    "            routeids.append(rows['routeid'])\n",
    "        else:\n",
    "            break\n",
    "df_journey = pd.DataFrame({'routeid':routeids,'segmentno':segmentnos,'segmentid':segmentids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the duplicate segments -> Queueing Theory\n",
    "df_journey.drop_duplicates(subset='segmentid',keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    StructField(\"tripid\",StringType(),True),\n",
    "    StructField(\"dayofservice\",StringType(),True),\n",
    "    StructField(\"stoppointid\",StringType(),True),\n",
    "    StructField(\"plannedtime_arr\",StringType(),True)\n",
    "]\n",
    "schema = StructType(fields)\n",
    "from_stop_df = sqlContext.createDataFrame(sc.emptyRDD(),schema)\n",
    "to_stop_df = sqlContext.createDataFrame(sc.emptyRDD(),schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journey.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_trips = df_trips_tidy.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = pdf_trips[['Trips_tripid','routeid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_39A_routes = fin_df['routeid'].loc[fin_df['routeid'].str.startswith('39A')].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_46A_routes =fin_df['routeid'].loc[fin_df['routeid'].str.startswith('46A')].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journey.sort_values(by='routeid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will run our loops:\n",
    "# First loop will iterate over the journey dataframe\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import LongType\n",
    "for i, rows in df_journey.iterrows():\n",
    "    route_trips_unq = []\n",
    "    segments = rows['segmentid']\n",
    "    route = rows['routeid']\n",
    "    if route in _39A_routes or route in _46A_routes:\n",
    "        route_trips_unq = fin_df[fin_df['routeid'].isin([route])]['Trips_tripid'].tolist()\n",
    "        print(i)\n",
    "        from_stop, to_stop = segments.split('_')\n",
    "        from_stop_df = sdf_stops_ordered.selectExpr('dayofservice','tripid','plannedtime_arr as time_at_1','stoppointid').where((sdf_stops_ordered.stoppointid==from_stop)&(sdf_stops_ordered.tripid.isin(route_trips_unq)))\n",
    "        to_stop_df = sdf_stops_ordered.selectExpr('dayofservice','tripid','plannedtime_arr as time_at_2','stoppointid').where((sdf_stops_ordered.stoppointid==to_stop)&(sdf_stops_ordered.tripid.isin(route_trips_unq)))\n",
    "        to_stop_df = to_stop_df.drop('stoppointid')\n",
    "        from_stop_df = from_stop_df.drop('stoppointid')\n",
    "        joined_df = from_stop_df.join(to_stop_df,['dayofservice','tripid'],'inner')\n",
    "        joined_df=joined_df.withColumn('segmentid',sf.lit(segments))\n",
    "        joined_df = joined_df.withColumn('traveltime',joined_df[\"time_at_2\"].cast(LongType())-joined_df[\"time_at_1\"].cast(LongType()))\n",
    "        sdf_stops_model = unionAll(sdf_stops_model,joined_df)\n",
    "    if route.startswith('5'):\n",
    "        break\n",
    "    # # condition = [from_stop_df.tripid == to_stop_df.tripid, from_stop_df.dayofservice==to_stop_df.dayofservice]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
